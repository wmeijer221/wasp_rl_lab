{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b17077a",
   "metadata": {},
   "source": [
    "# WASP RL - Meeting 2 - Lab assignment on NAF  and SQL\n",
    "\n",
    "In this lab we implement and experiment with NAF and SQL, both algorithms we learned about in the course and both can deal with continuous state **and** action spaces. Both algorithms employ deep neural networks for function approximation. To do this assignment, you will need to work with PyTorch, a popular framework for developing neural networks. When forming groups, make sure that at least one group member if comfortable with PyTorch and its most common tensor operations.\n",
    "\n",
    "This notebook has been created specifically for this course and this session by *Finn Rietz* and *Johannes A. Stork*, we hope you enjoy the lab :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6ad1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install requirements if needed\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install gym[classic_control]==0.26.2 numpy==1.23.0 matplotlib==3.1.2 \n",
    "# !{sys.executable} -m pip install torch==1.12.0 torchaudio==0.12.0 torchvision==0.13.0 scipy==1.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c74086c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch device: cpu\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kde\n",
    "import random\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.optim import Adam\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"torch device: {DEVICE}\")\n",
    "\n",
    "from buffer import ReplayBuffer\n",
    "from utils import presample_env, transform_action, smooth, obs_transform, OUNoise, rbf_kernel2\n",
    "from env_v2 import MultiGoalEnv\n",
    "from network import MLP\n",
    "from plotting import plot_loss, plot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84bfefab",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eafad94",
   "metadata": {},
   "source": [
    "# NAF\n",
    "We start with NAF. From the paper, we know that the value $V(\\mathbf{x} | \\theta)$, the action $\\mathbf{\\mu(x|}\\theta)$, and the matrix entries $\\mathbf{L(x}|\\theta)$ are being approximated by neural networks. Its easiest to just have one a single neural network with multiple output heads for each quantity. As a first step, **adjust the code below so that the network outputs the desired values**. This involves making output layers for each quantity and implementing the equations for $Q(x,u)|\\theta)$ and $A(x, u |\\theta)$ in the forward pass of the neural network:\n",
    "![NAF quantities](notebook_imgs/NAF_quantities.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe14dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAFNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, layer_size):\n",
    "        super(NAFNetwork, self).__init__()\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.fc_0 = nn.Linear(state_size, layer_size)\n",
    "        self.fc_1 = nn.Linear(layer_size, layer_size)\n",
    "        self.fc_2 = nn.Linear(layer_size, layer_size)\n",
    "        \n",
    "        # TODO: make output layers\n",
    "        # HINT: The number of entries in a (not-strictly) lower-triangular n x n matrix is: n * (n + 1) / 2\n",
    "        # HINT: These are just linear layers with the right number of outputs...\n",
    "        self.mu_head = None\n",
    "        self.v_head = None\n",
    "        self.mat_head = None\n",
    "\n",
    "    def forward(self, state, action=None):\n",
    "        \"\"\"\n",
    "        Forward pass of Normalized Advantage Function\n",
    "        Returns the noisy exploration action, Q(s, a), V(s), and the greedy, non-noisy action\n",
    "        \"\"\"\n",
    "        # get latent representation\n",
    "        x = torch.relu(self.fc_0(state))\n",
    "        x = torch.relu(self.fc_1(x))\n",
    "        x = torch.relu(self.fc_2(x))\n",
    "        \n",
    "        # TODO predict the action mu, the value v and the matrix entries\n",
    "        # HINT: Make use of the ouput heads we defined above\n",
    "        greedy_action = None\n",
    "        V = None\n",
    "        entries = None\n",
    "\n",
    "        greedy_action = greedy_action.unsqueeze(-1)\n",
    "        \n",
    "        # TODO: calculate P\n",
    "        # HINT: create empty lower-triangular matrix as described in the paper\n",
    "\n",
    "        # if an action is given (during batch update), calculate Q\n",
    "        Q = None\n",
    "        if action is not None:\n",
    "            # TODO: calculate Advantage:\n",
    "            A = None\n",
    "            Q = A + V\n",
    "            pass\n",
    "\n",
    "        # add noise to the greedy action, for exploration in continuous action space\n",
    "        noisy_action = None\n",
    "        \n",
    "        return noisy_action, Q, V, greedy_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210dc190",
   "metadata": {},
   "source": [
    "Next, we make a class for the NAF agent, that will hold an instance of the above class for the neural network. The agent also implements an update method that updates the parameters of the neural network networka and a rollout method. Fortunately, the NAF update is straightforward, since it directly minimizes the TD error. **In the below cell, in the `update` method, compute the loss as described in the algorithm**. The update method alongside the rollout method essentially implement the entire NAF algorithm:\n",
    "![NAF algorithm](notebook_imgs/NAF_algo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c461d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAFAgent:\n",
    "    def __init__(self, n_states, n_actions, buffer, net_size, gamma, tau, lr):\n",
    "        \"\"\"\n",
    "        @param n_states: The dimensionality of the state space\n",
    "        @param n_actions: The dimensionality of the action space\n",
    "        @param buffer: An instance of a replay buffer\n",
    "        @param net_size: The width of the NAF network layers\n",
    "        @param gamma: The discount rate\n",
    "        @param tau: The soft/polyak target network update rate\n",
    "        @param lr: The learning rate\n",
    "        \"\"\"\n",
    "        self.buffer = buffer\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "        self.update_counter = 0\n",
    "        self.GAMMA = gamma\n",
    "        self.TAU = tau\n",
    "        self.LR = lr\n",
    "        self.loss_hist = []\n",
    "\n",
    "        # main network\n",
    "        self.naf_net = NAFNetwork(\n",
    "            state_size=n_states,\n",
    "            action_size=n_actions,\n",
    "            layer_size=net_size,\n",
    "        )\n",
    "        self.naf_net.to(DEVICE)\n",
    "\n",
    "        # target network\n",
    "        self.target_net = NAFNetwork(\n",
    "            state_size=n_states,\n",
    "            action_size=n_actions,\n",
    "            layer_size=net_size,\n",
    "        )\n",
    "        self.target_net.to(DEVICE)\n",
    "        self.target_net.load_state_dict(self.naf_net.state_dict())\n",
    "\n",
    "        self.optim = Adam(self.naf_net.parameters(), lr=self.LR)\n",
    "        \n",
    "    def rollout(self, env, episode, mode=\"train\"):\n",
    "        \"\"\"\n",
    "        Runs one episode and does a batch update after each step.\n",
    "        \"\"\"\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not (done or truncated):\n",
    "            if mode == \"train\":\n",
    "                action, _, _, _ = self.naf_net(torch.from_numpy(obs).unsqueeze(0).to(DEVICE).to(torch.float32))\n",
    "            elif mode == \"eval\":\n",
    "                _, _, _, action = self.naf_net(torch.from_numpy(obs).to(DEVICE).to(torch.float32))\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid mode 'mode' given!\")\n",
    "            \n",
    "            action = transform_action(action.squeeze(0), env)\n",
    "            \n",
    "            new_obs, reward, done, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if mode == \"train\":\n",
    "                self.buffer.add(obs, action, reward, new_obs, done)\n",
    "                self.update()\n",
    "\n",
    "            if done or truncated:\n",
    "                print(f\"Total reward {mode} episode {episode}: {total_reward}\")\n",
    "                return total_reward\n",
    "            \n",
    "            obs = new_obs\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Updates the neural network based on a batch of experiences sampled from the replay buffer.\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < self.buffer.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample()\n",
    "        self.optim.zero_grad()\n",
    "        \n",
    "        # TODO: compute the loss\n",
    "        # HINT: Compute the Q prediction for the given states and actions\n",
    "        # HINT: Compute the target values\n",
    "        # --------------------- \n",
    "        loss = None\n",
    "                \n",
    "        # ---------------------\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.naf_net.parameters(), 1) \n",
    "        self.optim.step()\n",
    "\n",
    "        self.loss_hist.append(loss.item())\n",
    "        self.update_counter += 1\n",
    "\n",
    "        # polyak/soft target network udpate\n",
    "        for target_param, current_param in zip(self.target_net.parameters(), self.naf_net.parameters()):\n",
    "            target_param.data.copy_(self.TAU * current_param.data + (1.0 - self.TAU) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd2bb2",
   "metadata": {},
   "source": [
    "Now, we just need a method that initializes the agent and runs the main trianing loop . **In the next cell, don't change anything**. You can play with the hyperparameters later, but when we highly recommend that you don't change them for now, since the ones we put for you definitly work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625fe775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naf_algo(\n",
    "        env,\n",
    "        train_episodes=200,\n",
    "        net_size=64,\n",
    "        gamma=0.975,\n",
    "        tau=0.0025,\n",
    "        lr=0.001,\n",
    "        batch_size=128,\n",
    "        ): \n",
    "    \n",
    "    buffer = ReplayBuffer(\n",
    "        buffer_size=int(1e6),\n",
    "        batch_size=batch_size,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    \n",
    "    agent = NAFAgent(\n",
    "        env.observation_space.shape[0],\n",
    "        env.action_space.shape[0],\n",
    "        buffer,\n",
    "        net_size,\n",
    "        gamma,\n",
    "        tau,\n",
    "        lr\n",
    "    )\n",
    "    \n",
    "    # populate replay buffer with 10k random transitions\n",
    "    presample_env(env, agent.buffer, 10000)\n",
    "\n",
    "    # main loop\n",
    "    training_rewards = []\n",
    "    eval_rewards = []\n",
    "    eval_eps = []\n",
    "    for episode in range(train_episodes):\n",
    "        tr = agent.rollout(env, episode)\n",
    "        training_rewards.append(tr)\n",
    "\n",
    "        if episode % 5 == 0:\n",
    "            er = agent.rollout(env, episode, mode=\"eval\")\n",
    "            eval_rewards.append(er)\n",
    "            eval_eps.append(episode)\n",
    "            \n",
    "    plot_loss(agent.loss_hist)\n",
    "    plot_reward(training_rewards, eval_rewards, eval_eps)\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ffe96f",
   "metadata": {},
   "source": [
    "# Testing NAF on Pendulum environment\n",
    "Now, with all of this (partially boilerplate) code out of the way, its time to test our NAF agent. We use the [OpenAI Gym Pendulum environment](https://mgoulao.github.io/gym-docs/environments/classic_control/pendulum/), which is arguably one of the easiest, continuous action space environments. The agent should reach about -250 reward relatively quickly, if you've implemented everything correctly. There can still be quite some variation (i.e. between 0 and -700), depending on the random initialization of the pendulum..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad3fc15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pendulum_env = gym.make(\"Pendulum-v1\")\n",
    "naf_agent = naf_algo(pendulum_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa40531",
   "metadata": {},
   "source": [
    "### Continue with the next part of the assignment only when your NAF implementation maintains between 0 and -700 reward on the pendulum environment!\n",
    "Once the NAF agent reaches acceptable performance on the pendulum environment, explore the effect of exploration noise. In the NAF paper in section 8.2, the authors describe how to use the matrix $P$ from the advantage term for adaptive noise generation. Assuming you used simple Gaussian noise so far, go back to the the forward pass of the neural network and implement adaptive noise instead. Do you observe a considerable difference on the Pendulum environment when using adaptive noise?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0594968",
   "metadata": {},
   "source": [
    "# Multigoal environment\n",
    "Now, we turn to a more interesting but still toy example, the \"multigoal\" environment introduced in the Soft Q-Learning paper. This environment features four goals (the stars in the below image) the agent can navigate to. The observation is the current position, the actions are 2D velocities to apply to the agent. The agent always starts at the center (plus some small random offset). The reward is the distance to the closest goal plus a small cost proportional to the squarred action sum. Consider below the plot of the reward function evaluated at a fine grid of locations for further intuition:\n",
    "![multigoal env](notebook_imgs/multigoal-env-reward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d0dda3",
   "metadata": {},
   "source": [
    "# TASK: Draw on a paper a [contour plot](https://se.mathworks.com/help/examples/graphics/win64/DisplayContourLabelsExample_01.png) of what you think NAF's value function will look like. Keep the paper for later comparison with the actual value function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b370fb9",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdc73f8",
   "metadata": {},
   "source": [
    "# TASK: Also draw the trajectories that you think NAF's policy will generate. Keep the paper for later comparison with actual trajectories generated by the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31013762",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fe717f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multigoal_env = MultiGoalEnv()\n",
    "naf_agent = naf_algo(multigoal_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ed727",
   "metadata": {},
   "source": [
    "# Visualizing the value function\n",
    "In addition to the the loss plot and reward plot above, it can be helpfull to visualize the learned value function and or policy, to understand better what the agent learned. This is often straightforward in 2D environments, since a small 2D grid is still managable in terms of compute. As such, **visualize the value function learned by the NAF agent on the multigoal environment**. Does NAF's learned value function look similar to what you drew on the paper? Can you explain why the learned value function looks the way it does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb618e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7c019",
   "metadata": {},
   "source": [
    "# Visualizing the Q-function\n",
    "Also plot the Q-function NAF learned for the environment state [0, 0], i.e. with the agent placed in the middle of the environment. Does the Q-function match the value function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff373a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7805e172",
   "metadata": {},
   "source": [
    "# Visualizing trajectories\n",
    "In addition to the value function, we can inspect the trajectories generated by the policy. **Collect, plot and analyze some (10 - 50) trajectories from the policy**. Are the agent's trajectory similar to what you drew on the paper? Explain the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecddb378",
   "metadata": {},
   "outputs": [],
   "source": [
    "multigoal_env.init_sigma = 0.0  # disable reset noise, only used to help exploration during learning...\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11de1f1a",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4153ee7f",
   "metadata": {},
   "source": [
    "# Soft Q-Learning\n",
    "\n",
    "\n",
    "We now turn to the second algorithm we learned about that can deal with continuous state and action spaces: Soft Q-learning (SQL). SQL is based on two neural networks, one for the $Q$-function and one for the policy. These are plain MLPs, hence this time you don't have to implement the neural network. However, you must initialize these networks with the correct input and output sizes. Thus, **in the `init` method in the next cell, set the `q_in_size`, `q_out_size`, `asvgd_noise_size`, `pi_in_size`, and `pi_out_size` variables**. These are passed to the respective neural networks initalizations.\n",
    "\n",
    "Further, you will implement the update of the $Q$-function approximating neural network. This is relatively straightforward and based on the TD-error. **In the next cell, in the `td_update` method, calculate the loss for the $Q$-approximating network**. For this, you need to implement and consider the following equations. The soft value function:\n",
    "\n",
    "![sql_vsoft](notebook_imgs/SQL_v_soft.png)\n",
    "\n",
    "The soft value function's empirical estimate:\n",
    "\n",
    "![sql_empirical_vsoft](notebook_imgs/SQL_empirical_v_soft.png)\n",
    "\n",
    "And the minimization objective:\n",
    "\n",
    "![sql_JQ](notebook_imgs/SQL_jq.png)\n",
    "\n",
    "Note, that $\\hat{Q}^{\\bar\\theta}_{\\text{soft}} = r_t + \\gamma \\mathbb{E}_{s_{t+1}\\sim p_s}[ V^{\\bar\\theta}_{\\text{soft}}(s_{t+1}) ]$ and that $\\bar\\theta$ refers to the target network parameter.\n",
    "\n",
    "We implement the update of the policy network for you, because itrelies on a method not covered in the course, the Amortized Stein Variational Gradient Descent (ASVGD). ASVGD has gained increasing popularity in SOTA research and provides an interesting alternative to sampling methods like Markov Chain Monte Carlo or Metropolis Hastings. ASVGD moves a set of random particles such that, after convergence, they act like samples from the distribution of interest, and only requires access to the unnormalized density. You can take a look how this is implemented and how it updates the policy network in the `asvgd_update` method. If you are interested, we encourage you to take a look at the official [project website](https://www.cs.utexas.edu/~qlearning/project.html?p=svgd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7465230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLAgent:\n",
    "    \"\"\"\n",
    "    SQL agent, https://arxiv.org/pdf/1702.08165.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_states,\n",
    "                 n_actions,\n",
    "                 buffer,\n",
    "                 net_size,\n",
    "                 gamma,\n",
    "                 q_lr,\n",
    "                 asvgd_lr,\n",
    "                 hard_freq,\n",
    "                 reward_scale,\n",
    "                 n_particles,\n",
    "                 sampler_act_fun=torch.tanh,\n",
    "                 particle_scale=10.0,\n",
    "                 bandwidth_scale=1\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        @param n_states: The dimensionality of the state space\n",
    "        @param n_actions: The dimensionality of the action space\n",
    "        @param buffer: An instance of a replay buffer\n",
    "        @param net_size: The width of the NAF network layers\n",
    "        @param gamma: The discount rate\n",
    "        @param q_lr: The learning rate for the q network\n",
    "        @param asvgd_lr: The learnign rate for the policy network\n",
    "        @param hard_freq: The hard update frequency of the q target network\n",
    "        @param reward_scale: The scale of the reward signal\n",
    "        @param n_particles: The number of particles used by ASVGD\n",
    "        @param sampler_act_fun: The activation function used by the sampling network\n",
    "        @param particle_scale: The scale of the particles noise for SVGD sampler\n",
    "        \"\"\"\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.buffer = buffer\n",
    "        # self.loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "        self.loss_fn = nn.SmoothL1Loss(reduction=\"mean\")\n",
    "        self.q_update_counter = 0\n",
    "        self.GAMMA = gamma\n",
    "        self.HARD_FREQ = hard_freq\n",
    "        self.loss_hist = []\n",
    "        self.svgd_loss_hist = []\n",
    "        self.sample_diversity_hist = []\n",
    "        self.reward_scale = reward_scale\n",
    "        self.noise_gen = OUNoise(n_actions)\n",
    "        self.n_particles = n_particles\n",
    "        self.particle_scale = particle_scale\n",
    "        self.bandwidth_scale = bandwidth_scale\n",
    "        \n",
    "        #  TODO: set these to the right values (these are all integers...)\n",
    "        # ---------------------\n",
    "        q_in_size = None\n",
    "        q_out_size = None\n",
    "        asvgd_noise_size = None\n",
    "        pi_in_size = None\n",
    "        pi_out_size = None\n",
    "        # ---------------------\n",
    "\n",
    "        # Q network\n",
    "        self.q_net = MLP(\n",
    "            in_size=q_in_size,\n",
    "            out_size=q_out_size,\n",
    "            layer_size=net_size,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # target Q network\n",
    "        self.target_q_net = MLP(\n",
    "            in_size=q_in_size,\n",
    "            out_size=q_out_size,\n",
    "            layer_size=net_size,\n",
    "        ).to(DEVICE)\n",
    "        self.target_q_net.to(DEVICE)\n",
    "        self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        # policy sampling network\n",
    "        self.pi_net = MLP(\n",
    "            in_size=pi_in_size,\n",
    "            out_size=pi_out_size,\n",
    "            layer_size=net_size,\n",
    "            act_fun=sampler_act_fun\n",
    "        ).to(DEVICE)\n",
    "        self.asvgd_noise_size = asvgd_noise_size\n",
    "\n",
    "        self.q_optim = Adam(self.q_net.parameters(), lr=q_lr, weight_decay=0.01)\n",
    "        self.asvgd_optim = Adam(self.pi_net.parameters(), lr=asvgd_lr)\n",
    "\n",
    "    def td_update(self):\n",
    "        \"\"\"\n",
    "        Updates the Q network based on a batch of experiences drawn uniformly from the replay buffer\n",
    "        \"\"\"\n",
    "        if len(self.buffer.memory) < self.buffer.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample()\n",
    "        self.q_optim.zero_grad()\n",
    "\n",
    "        # predict q for batch\n",
    "        q = self.q_net.forward(torch.cat((states, actions), dim=1))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # TODO, calculate the 'target' aka 'desired' value J_Q(\\theta), so that we can minimize the loss \n",
    "            # between q and the target\n",
    "            # ---------------------\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            target = None\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # ---------------------\n",
    "        \n",
    "        \n",
    "        loss = self.loss_fn(q, target)\n",
    "        loss.backward()\n",
    "        self.q_optim.step()\n",
    "        self.loss_hist.append(loss.item())\n",
    "\n",
    "        self.q_update_counter += 1\n",
    "\n",
    "        # hard target network update\n",
    "        if self.q_update_counter % self.HARD_FREQ == 0:\n",
    "            self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "            \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Helper function that constructs the input to the policy network.\n",
    "        This is done slightly differently depending on on the shape of the state tensor.\n",
    "        \"\"\"\n",
    "        if state.shape[0] == 1:\n",
    "            # environment interaction\n",
    "            zeta = torch.rand(self.n_particles, self.asvgd_noise_size).to(DEVICE)\n",
    "            state = state.repeat(self.n_particles, 1)\n",
    "            inp = torch.cat((state, zeta), dim=1)\n",
    "        else:\n",
    "            # batch update\n",
    "            zeta = torch.rand(state.shape[0], self.n_particles, self.asvgd_noise_size).to(DEVICE)\n",
    "            state = state.repeat(self.n_particles, 1, 1).movedim(1, 0)\n",
    "            inp = torch.cat((state, zeta), dim=2)\n",
    "\n",
    "        action = self.pi_net.forward(inp.to(torch.float32))\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def rollout(self, env, episode, mode=\"train\"):\n",
    "        \"\"\"\n",
    "        Runs one episode and does a batch update after each step.\n",
    "        \"\"\"\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not (done or truncated):\n",
    "            a = self.act(torch.from_numpy(obs).unsqueeze(0).to(torch.float32).to(DEVICE))\n",
    "            o_stack = obs_transform(torch.from_numpy(obs).unsqueeze(0), self.n_particles).to(torch.float32).to(DEVICE)\n",
    "            q = self.q_net.forward(torch.cat((o_stack, a), dim=1))\n",
    "            ind = torch.argmax(q)\n",
    "            action = a[ind]\n",
    "\n",
    "            if mode == \"train\":\n",
    "                action += torch.from_numpy(self.noise_gen.sample()).to(DEVICE)\n",
    "\n",
    "            action = transform_action(action, env)\n",
    "            new_obs, reward, done, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if mode == \"train\":\n",
    "                self.buffer.add(obs, action, reward, new_obs, done)\n",
    "                self.td_update()\n",
    "                self.asvgd_update()\n",
    "\n",
    "            if done or truncated:\n",
    "                print(f\"Total reward {mode} episode {episode}: {total_reward}\")\n",
    "                return total_reward\n",
    "\n",
    "            obs = new_obs\n",
    "\n",
    "    def asvgd_update(self):\n",
    "        \"\"\"\n",
    "        Updates the policy network using the ASVGD method on a batch of experiences drawn uniformly from the \n",
    "        replay buffer.\n",
    "        \"\"\"\n",
    "        if len(self.buffer.memory) < self.buffer.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample()\n",
    "\n",
    "        # as described in the appendix of SQL paper, we need two sets of actions\n",
    "        actions = self.act(states)\n",
    "        assert actions.shape == (states.shape[0], self.n_particles, self.n_actions)\n",
    "        \n",
    "        fixed_actions = self.act(states)\n",
    "        fixed_actions = fixed_actions.detach()\n",
    "        fixed_actions.requires_grad = True\n",
    "\n",
    "        # just for analysis, compute diversity of samples\n",
    "        with torch.no_grad():\n",
    "                pairwise_dists = torch.cdist(actions, fixed_actions, p=2)\n",
    "                diversity_score = torch.mean(torch.exp(-pairwise_dists))\n",
    "                self.sample_diversity_hist.append(diversity_score.item())\n",
    "\n",
    "        # target for sampler is q, aka unnormalized policy density, thanks to maximum entropy framework\n",
    "        state_stack = obs_transform(states, self.n_particles).to(DEVICE)\n",
    "        asvgd_target = self.q_net(torch.cat((state_stack, fixed_actions), dim=2))\n",
    "        log_p = asvgd_target\n",
    "\n",
    "        grad_log_p = torch.autograd.grad(log_p.sum().to(DEVICE), fixed_actions.to(DEVICE))[0]\n",
    "        grad_log_p = grad_log_p.unsqueeze(1)\n",
    "\n",
    "        kappa, kappa_grad = rbf_kernel2(actions, fixed_actions, h_scale=self.bandwidth_scale)\n",
    "        \n",
    "        # eq 13 in paper, stein gradient\n",
    "        # actions_grad = (1/self.n_particles) * torch.sum(kappa * grad_log_p + kappa_grad, dim=1)\n",
    "        actions_grad = (1/self.n_particles) * torch.mean(kappa * grad_log_p + kappa_grad, dim=1)  # this is a lot better!\n",
    "        actions_grad.to(DEVICE)\n",
    "        # print(\"actions_grad.shape\", actions_grad.shape, \"actions.shape\", actions.shape)\n",
    "        self.svgd_loss_hist.append(actions_grad.sum(dim=-1).sum(dim=1).mean(dim=0).item())\n",
    "\n",
    "        self.asvgd_optim.zero_grad()\n",
    "        torch.autograd.backward(-actions, grad_tensors=actions_grad)  # this implements eq 14, chain rule backprop\n",
    "        self.asvgd_optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84378d8d",
   "metadata": {},
   "source": [
    "With the update methods taken care of, as before, the only thing that is missing is the main loop. Also as before, we high suggest you don't change any of the hyperparameters until your SQL implementation is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bfa39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_algo(\n",
    "        env, \n",
    "        reward_scale=10, \n",
    "        n_particles=128, \n",
    "        train_episodes=200,\n",
    "        batch_size=64,\n",
    "        net_size=128,\n",
    "        gamma=0.99,\n",
    "        q_lr=0.001,\n",
    "        pi_lr=0.0001,\n",
    "        hard_freq=1000,\n",
    "        particle_scale=10,\n",
    "        bandwidth_scale=1,\n",
    "        ):\n",
    "    \n",
    "    buffer = ReplayBuffer(\n",
    "        buffer_size=int(1e6),\n",
    "        batch_size=batch_size,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    \n",
    "    agent = SQLAgent(\n",
    "        env.observation_space.shape[0],\n",
    "        env.action_space.shape[0],\n",
    "        buffer,\n",
    "        net_size,\n",
    "        gamma,\n",
    "        q_lr,\n",
    "        pi_lr,\n",
    "        hard_freq,\n",
    "        reward_scale,\n",
    "        n_particles,\n",
    "        particle_scale=particle_scale,\n",
    "        bandwidth_scale=bandwidth_scale\n",
    "    )\n",
    "    \n",
    "    # populate replay buffer with 10k random transitions\n",
    "    presample_env(env, agent.buffer, 10000)\n",
    "\n",
    "    # main loop\n",
    "    training_rewards = []\n",
    "    eval_rewards = []\n",
    "    eval_eps = []\n",
    "    for episode in range(train_episodes):\n",
    "        print(f\"Episode {episode}...\\r\", end=\"\")      \n",
    "        tr = agent.rollout(env, episode)\n",
    "        training_rewards.append(tr)\n",
    "\n",
    "        if episode % 5 == 0:\n",
    "            er = agent.rollout(env, episode, mode=\"eval\")\n",
    "            eval_rewards.append(er)\n",
    "            eval_eps.append(episode)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    plot_loss(agent.loss_hist, ax=axs[0])\n",
    "    axs[0].set_title(\"Training Loss Q\")\n",
    "    plot_loss(agent.svgd_loss_hist, ax=axs[1])\n",
    "    axs[1].set_title(\"Training Loss SVGD\")\n",
    "    plot_loss(agent.sample_diversity_hist, ax=axs[2])\n",
    "    axs[2].set_title(\"Sampler diversity\")\n",
    "    plot_reward(training_rewards, eval_rewards, eval_eps, ax=axs[3])\n",
    "    axs[3].set_title(\"Reward\")\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1188095",
   "metadata": {},
   "source": [
    "# Testing SQL on Pendulum environment\n",
    "As before, we first evaluate our implementation on the pendulum environment. The algorithm should be able to reach good performance (between 0 and -700) reward within 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59120c10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pendulum_env = gym.make(\"Pendulum-v1\")\n",
    "sql_agent = sql_algo(pendulum_env, reward_scale=10, n_particles=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc63bdf",
   "metadata": {},
   "source": [
    "# Testin SQL on Multigoal environment\n",
    "Once you have verified your SQL implementation, you can continue and test it on the multigoal environment.\n",
    "\n",
    "## Again, draw what you think the value function learned by SQL will look like. Also draw the trajectories you think SQL will generate.\n",
    "Again keep the paper for later comparison with the actual results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca46cd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multigoal_env = MultiGoalEnv()\n",
    "sql_agent = sql_algo(multigoal_env, n_particles=128, train_episodes=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67240080",
   "metadata": {},
   "source": [
    "# Visualizing the value function\n",
    "As before, **visualze the learned value function of the SQL agent**. How does it relate to the reward landscape we saw earlier? How does it compare to the value function learned by NAF? How does it compare to the value function you drew on the paper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad69885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ee8f8",
   "metadata": {},
   "source": [
    "# Visualizing the Q-function\n",
    "Plot the Q-function SQL learned at state [0,0]. Does it matcht the value function? How does it compare to the Q-function NAF learned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "004a3439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26913d7a",
   "metadata": {},
   "source": [
    "# Visualizing the policy\n",
    "Now, lets also try to **visualize the policy network at interesting points in the environment**. These points coulde be $(0, 0), (\\pm 2, 0),$ or $(\\pm2.5, \\pm2.5)$. Construct the observation manually, feed it into the policy network and visualize the particles. Bonus points if you run KDE on the generated particles ;)\n",
    "What do you observe? How does the plot of the particles and the KDE explain the shortcomming of the NAF algorithm on the multigoal environment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4c3c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1b9cf7",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bf85fc",
   "metadata": {},
   "source": [
    "# Visualizing trajectories\n",
    "As we did before with NAF, lets explore trajectories taken by the learned SQL policy. **Collect, plot and analyze some (10 - 50) trajectories generated by the SQL agent**. Are the trajectories similar to what you drew on the paper? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6dc757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e183aeaa",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707f4a29",
   "metadata": {},
   "source": [
    "# Optional tasks:\n",
    "If you are done with everything and there is time left, we encourage you to do some (or all) of the following tasks:\n",
    "+ Play with hyperparameters: For SQL, particularly interesting might be the reward scale, the number of particles used by the policy network with ASVGD, and the learning rates `Q_LR` and `PI_LR` (they might have an unespected relationship).\n",
    "+ Generally, varying the batch size, the network size, the loss functions, and the optimizers target update rate `TAU` or hard update frequncy `HARD_FREQ` can have intersting effects. A poor value can often render the RL algorithm completely incapable of learning a task, while a good value can speed up and stabilize training considerably. Try varying those parameters and note the effects.\n",
    "+ You can implement different explortation mechanisms. For example, the SQL implementation uses OU noise, which is a a form of temporally correlated noise and a rather powerful mechanism. How does SQL perform with, for example, epsilon greedy or Gaussian noise? In the same way, perhaps NAF can be improved by using OU noise instead of Gaussian or adaptive noise?\n",
    "+ Lastly, given the somewhat complex ASVGD update mechanism in the SQL policy update, you can implement a simpler method like Metropolis Hastings or Hamiltonian Monte Carlo to sample from the policy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
